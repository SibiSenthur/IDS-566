{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning\n",
    "\n",
    "## Using keras package:\n",
    "\n",
    "Open a shell and install Keras.\n",
    "\n",
    "For anaconda: conda install keras\n",
    "\n",
    "For standard python: pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../Lecture-4/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n",
    "\n",
    "#First shuffle data\n",
    "df = df.sample(frac=1, random_state= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text = df['review'][:20000]\n",
    "y_train = df['sentiment'][:20000]\n",
    "#y_train = keras.utils.to_categorical(y_train)\n",
    "X_valid_text = df['review'][20000:]\n",
    "y_valid = df['sentiment'][20000:]\n",
    "\n",
    "print(y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vecotizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 16414)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#vect = CountVectorizer(binary = False, min_df = 10)\n",
    "vect = TfidfVectorizer(min_df = 10)\n",
    "\n",
    "X_train = vect.fit_transform(X_train_text)\n",
    "print(X_train.shape)\n",
    "X_valid = vect.transform(X_valid_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logistic regression as the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics  import accuracy_score\n",
    "\n",
    "LR = LogisticRegression(C = 4.1)\n",
    "LR.fit(X_train, y_train)\n",
    "p_valid = LR.predict(X_valid)\n",
    "print(accuracy_score(y_valid, p_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 2 layers MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 32)                525280    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model using SGD with learning rate as .1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a binary classification problem\n",
    "opt = keras.optimizers.SGD(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with batch_size = 1024 for 5 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 6s 299us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.5954 - val_acc: 0.8762\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 5s 262us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.5958 - val_acc: 0.8762\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 6s 279us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.5962 - val_acc: 0.8762\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 5s 274us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.5966 - val_acc: 0.8762\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 5s 262us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.5971 - val_acc: 0.8760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2360ae07a90>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 6s 305us/step - loss: 0.3914 - acc: 0.8208 - val_loss: 0.2948 - val_acc: 0.8784\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 5s 275us/step - loss: 0.1161 - acc: 0.9588 - val_loss: 0.3399 - val_acc: 0.8818\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 5s 275us/step - loss: 0.0344 - acc: 0.9907 - val_loss: 0.4391 - val_acc: 0.8728\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 6s 282us/step - loss: 0.0077 - acc: 0.9992 - val_loss: 0.5514 - val_acc: 0.8722\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 6s 283us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.6155 - val_acc: 0.8724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23612bf6dd8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "# For a binary classification problem\n",
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tanh as activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 7s 327us/step - loss: 0.4583 - acc: 0.7948 - val_loss: 0.2864 - val_acc: 0.8822\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 6s 285us/step - loss: 0.1457 - acc: 0.9476 - val_loss: 0.3218 - val_acc: 0.8804\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 5s 266us/step - loss: 0.0626 - acc: 0.9829 - val_loss: 0.4329 - val_acc: 0.8744\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 5s 268us/step - loss: 0.0220 - acc: 0.9957 - val_loss: 0.5581 - val_acc: 0.8676\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 5s 267us/step - loss: 0.0074 - acc: 0.9990 - val_loss: 0.6667 - val_acc: 0.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2360bc7bac8>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation = 'tanh', input_shape=(X_train.shape[1],)),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "# For a binary classification problem\n",
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dropout layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 6s 305us/step - loss: 0.4928 - acc: 0.7442 - val_loss: 0.2797 - val_acc: 0.8854\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 5s 258us/step - loss: 0.3453 - acc: 0.8389 - val_loss: 0.2657 - val_acc: 0.8942\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 5s 259us/step - loss: 0.3081 - acc: 0.8537 - val_loss: 0.2585 - val_acc: 0.8934\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 5s 261us/step - loss: 0.2868 - acc: 0.8678 - val_loss: 0.2582 - val_acc: 0.8954\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 5s 267us/step - loss: 0.2503 - acc: 0.8854 - val_loss: 0.2596 - val_acc: 0.8926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2361b3e1a90>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(.8),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "# For a binary classification problem\n",
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 8s 409us/step - loss: 0.5900 - acc: 0.6780 - val_loss: 0.3618 - val_acc: 0.8718\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 7s 344us/step - loss: 0.4587 - acc: 0.7716 - val_loss: 0.3108 - val_acc: 0.8912\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 7s 338us/step - loss: 0.4158 - acc: 0.7992 - val_loss: 0.2909 - val_acc: 0.8910\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 7s 333us/step - loss: 0.3839 - acc: 0.8158 - val_loss: 0.2715 - val_acc: 0.8902\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 7s 338us/step - loss: 0.3792 - acc: 0.8222 - val_loss: 0.2655 - val_acc: 0.8876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x236279460f0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(.9),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "# For a binary classification problem\n",
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 8s 400us/step - loss: 0.6289 - acc: 0.6315 - val_loss: 0.3549 - val_acc: 0.8510\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 7s 349us/step - loss: 0.4073 - acc: 0.8224 - val_loss: 0.2847 - val_acc: 0.8832\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 7s 348us/step - loss: 0.3432 - acc: 0.8529 - val_loss: 0.2673 - val_acc: 0.8920\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 7s 350us/step - loss: 0.3048 - acc: 0.8726 - val_loss: 0.2658 - val_acc: 0.8900\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 7s 343us/step - loss: 0.2850 - acc: 0.8824 - val_loss: 0.2607 - val_acc: 0.8898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2361e227320>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(.8),\n",
    "    Dense(8, activation = 'tanh'),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "# For a binary classification problem\n",
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNN with LSTM layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid) = imdb.load_data(num_words = 10000, start_char = 1, oov_char = 2, index_from = 3)\n",
    "imdb_vocab = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {value + 3: key for key, value in imdb_vocab.items()}\n",
    "vocab[1] = 'START'\n",
    "vocab[2] = 'OOV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert OOV is an amazing actor and now the same being director OOV father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for OOV and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also OOV to the two little boy's that played the OOV of norman and paul they were just brilliant children are often left out of the OOV list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([vocab[x] for x in X_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 63s 3ms/step - loss: 0.5233 - acc: 0.7507 - val_loss: 0.4203 - val_acc: 0.8132\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 61s 2ms/step - loss: 0.3737 - acc: 0.8426 - val_loss: 0.4051 - val_acc: 0.8170\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.3156 - acc: 0.8722 - val_loss: 0.4137 - val_acc: 0.8116\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 57s 2ms/step - loss: 0.2673 - acc: 0.8963 - val_loss: 0.4130 - val_acc: 0.8223\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.2300 - acc: 0.9111 - val_loss: 0.4308 - val_acc: 0.8204\n",
      "Epoch 6/15\n",
      "  576/25000 [..............................] - ETA: 1:01 - loss: 0.1618 - acc: 0.9323"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-293-dce5c5d719f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, GlobalAveragePooling1D\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, output_dim = 32),\n",
    "    #LSTM(8, dropout=0.2, recurrent_dropout=0.2),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "# For a binary classification problem\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
